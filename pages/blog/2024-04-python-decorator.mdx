---
title: "Trace complex LLM applications with the Langfuse decorator (Python)"
date: 2024/04/22
description: When building RAG or agents, lots of LLM calls and non-LLM inputs feeds into the final output. The Langfuse decorator allows you to trace and evaluate it holistically.
ogImage: /images/changelog/2024-03-24-python-decorator.png
tag: integration
author: Marc
---

import { BlogHeader } from "@/components/blog/BlogHeader";

<BlogHeader
  title="Trace complex LLM applications with the Langfuse decorator (Python)"
  description="When building RAG or agents, lots of LLM calls and non-LLM inputs feeds into the final output. The Langfuse decorator allows you to trace and evaluate it holistically."
  date="April 21, 2024"
  authors={["marcklingen", "hassiebpakzad"]}
/>

When our team initially built complex agents for web scraping and code generation while in Y Combinator [1], we quickly recognized the need for new LLM-focused observability to truly understand how our applications produced their outputs. It wasn't just about the LLM calls, but also the retrieval steps, chaining of separate calls, function calling, obtaining the right JSON output, and various API calls that the agents should perform behind the scenes.

This insight prompted us to start working on Langfuse. As many teams were experimenting, we prioritized easy integrations with frameworks like LangChain and LlamaIndex, as well as wrapping the OpenAI SDK to log individual LLM calls. As applications become increasingly complex and agents are deployed in production, the ability to trace complex applications is even more crucial.

[Traces](/docs/tracing) are at the core of the Langfuse platform. Until now, generating a trace was straightforward if you used a framework. However, if you didn't, you had to use the low-level SDKs to manually create and nest trace objects, which added verbose instrumentation code to a project. Inspired by the developer experience of tools we admire, such as Sentry and Modal, we created the `@observe()` decorator for Langfuse to make tracing your Python code as simple as possible.

This post is a deep dive into the `@observe()` decorator, our design objectives, challenges we faced when implementing it, and how it can help you trace complex LLM applications.

## Goals

When starting to work on the decorator, we wanted to make everything simple that's complex when manually instrumenting your code. The decorator should:

- Trace all function calls and their outputs in a single trace
- Be fully compatible with the native [integrations](/docs/integrations) with LangChain, LlamaIndex, and the OpenAI SDK
- Be easy to use and require minimal changes to your code
- Automatically capture the function name, arguments, return value, exceptions, execution time, and nesting of functions
- Encourage reusable abstractions across an LLM-based application without needing to consider how to pass trace objects around

## Design decisions

1. The decorator reuses the low-level SDK to create traces and asynchronously batch them to the Langfuse API. This implementation was derived from the PostHog SDKs and is [tested](/guides/cookbook/langfuse_sdk_performance_test) to have little to no impact on the performance of your application.
2. The decorator uses context managers to handle the nesting of functions and the creation of spans. This way, you can easily see which function calls are nested within others. Another popular implementation is to add uuids to the stack trace, but we did not want to pollute the stack trace.
3. Use the same `observe()` decorator to create a trace (outermost function) and to add spans to the trace (inner functions). This way, functions can be used in multiple traces without needing to be strictly a "trace" or a "span" function.

## Before and after

### Status quo: Low-level SDK

The low-level SDK is very flexible but it is also _very_ verbose and requires passing of Langfuse objects ðŸ‘‡

```python /langfuse = Langfuse()/ /span = trace.span(name="story")/ /trace_id=trace.id/ /parent_observation_id=span.id/ /span.end(output=output)/ /trace = langfuse.trace("main")/ /trace/
from langfuse import Langfuse
from langfuse.openai import openai # OpenAI integration

langfuse = Langfuse()

def story(trace):
  span = trace.span(name="story")
  output = openai.chat.completions.create(
        model="gpt-3.5-turbo",
        max_tokens=100,
        messages=[
          {"role": "system", "content": "You are a great storyteller."},
          {"role": "user", "content": "Once upon a time in a galaxy far, far away..."}
        ],
        trace_id=trace.id,
        parent_observation_id=span.id
    ).choices[0].message.content
  span.end(output=output)
  return output


def main():
  trace = langfuse.trace("main")
  return story(trace)
```

### `@observe()` decorator to the rescue

All complexity is abstracted away and you can focus on your business logic ðŸ‘‡ The OpenAI SDK wrapper is aware that it is run within a decorated function and automatically adds its logs to the trace.

```python /@observe()/
from langfuse.decorators import observe
from langfuse.openai import openai # OpenAI integration

@observe()
def story():
    return openai.chat.completions.create(
        model="gpt-3.5-turbo",
        max_tokens=100,
        messages=[
          {"role": "system", "content": "You are a great storyteller."},
          {"role": "user", "content": "Once upon a time in a galaxy far, far away..."}
        ],
    ).choices[0].message.content

@observe()
def main():
    return story()

main()
```

<Frame fullWidth>
  ![Simple OpenAI decorator
  trace](/images/docs/python-decorator-simple-trace.png)
</Frame>

## Interoperability

The decorator completely replaces the need to use the low-level SDK. It allows for the creation and manipulation of traces, and you can add custom scores and evaluations to these traces as well. Have a look at the extensive [documentation](/docs/sdk/python/decorators) for more details.

**Langfuse is natively integrated with LangChain, LlamaIndex, and the OpenAI SDK and the decorator is [fully compatible](/docs/sdk/python/example#interoperability-with-other-integrations) with these integrations.** As a result, you can, in a single trace, use the decorator on the outermost function, decorate function calls and API calls that are non-LLM related, and use the native instrumentation for the OpenAI SDK, LangChain and Llama Index.

Example:

```python
from langfuse.openai import openai
from langfuse.decorators import observe

@observe()
def openai_fn(calc: str):
    res = openai.chat.completions.create(
        model="gpt-3.5-turbo",
        messages=[
          {"role": "system", "content": "You are a very accurate calculator. You output only the result of the calculation."},
          {"role": "user", "content": calc}],
    )
    return res.choices[0].message.content

@observe()
def llama_index_fn(question: str):
    # Set callback manager for LlamaIndex, will apply to all LlamaIndex executions in this function
    langfuse_handler = langfuse_context.get_current_llama_index_handler()
    Settings.callback_manager = CallbackManager([langfuse_handler])

    # Run application
    index = VectorStoreIndex.from_documents([doc1,doc2])
    response = index.as_query_engine().query(question)
    return response

@observe()
def langchain_fn(person: str):
    # Get Langchain Callback Handler scoped to the current trace context
    langfuse_handler = langfuse_context.get_current_langchain_handler()

    # Pass handler to invoke method of chain/agent
    chain.invoke({"person": person}, config={"callbacks":[langfuse_handler]})

@observe()
def main():
    output_openai = openai_fn("5+7")
    output_llamaindex = llama_index_fn("What did he do growing up?")
    output_langchain = langchain_fn("Feynman")

    return output_openai, output_llamaindex, output_langchain

main();
```

## Outlook

The decorator drive open tracing for teams that don't want to commit to a single application framework or ecosystem, but want to easily switch between frameworks while still Langfuse as a single [platform](/docs) for all experimentation, observability and evaluation needs.

Roadmap: The decorator is currently only available for Python. We are working on a similar implementation for JS/TS. The decorator currently does not support class methods. We are working on a solution for this.

## Thank you

Big thanks to everyone who tried the decorator while it was in beta and provided feedback. We've received multiple Gists of potential implementations by community members of their own integrations before getting the decorator to the current state. We're excited to see what you build with it!

## Get Started

Run the end-to-end cookbook on your Langfuse traces or learn more about model-based evals in Langfuse.

import { FileCode, BookOpen } from "lucide-react";

<Cards num={3}>
  <Card title="Docs" href="/docs/sdk/python/decorators" icon={<BookOpen />} />
  <Card
    title="Example notebook"
    href="/docs/sdk/python/example"
    icon={<FileCode />}
  />
</Cards>
